% CSC722 HW11
% Jon Craton

> The whole work for homework and finals are required. The final results are not acceptable.

> Apply the gradient descent algorithm for the following function for 3 iterations assuming learning rate alpha = 0.01 

Problem 1
=========

$f(x) = x^2 + 1$

As the name implies, the first step to gradient descent is calculating our gradient. For this expression that is:

$f(x) = x^2 + 1$

$f'(x) = 2x$

Initial Guess
-------------

Now, we pick a point at random. Let's start from x=.6. 

Iteration 1
-----------

We plug our current value into our derivative to get the gradient at that point:

$f'(.6) = 2(.6) = 1.2$

Then we adjust our guess based on the local gradient and learning rate as:

$x_0 = .6$

$x_{n+1} = x_n - αf'(x_n)$

$α=.01$

$x_1 = .6 - (.01)(1.2)$

```python
.6 - .01*1.2
```

Iteration 2
-----------

We plug our new value into our derivative to get the gradient at that point:

$f'(.588) = 2(.588) = 1.176$

Then we adjust our guess based on the local gradient and learning rate as:

$x_1 = .588$

$x_{n+1} = x_n - αf'(x_n)$

$α=.01$

$x_2 = .588 - (.01)(1.176)$

```python
.588 - (.01)*(1.176)
```

Iteration 3
-----------

We plug our new value into our derivative to get the gradient at that point:

$f'(.57624) = 2(.57624) =  1.15248$

Then we adjust our guess based on the local gradient and learning rate as:

$x_1 = .57624$

$x_{n+1} = x_n - αf'(x_n)$

$α=.01$

$x_2 = .57624 - (.01)(1.15248)$

```python
.57624 - (.01)*(1.15248)
```

This shows the basic concept after three iterations. We'd obviously need to continue to get close to our real minimum.

Python Implementation
---------------------

Let's implement this algorithm in Python to solidify our understanding.

```python
from sympy import Symbol, diff, init_printing
init_printing(use_unicode=True)

x = Symbol('x')

def gradient_descent_step(f, x, x_0, α):
  """
  Perform one step of gradient descent

  >>> "%.06f" % gradient_descent_step(x**2 + 1, x, .6, .01)
  '0.588000'
  >>> "%.06f" % gradient_descent_step(x**2 + 1, x, .588, .01)
  '0.576240'
  >>> "%.06f" % gradient_descent_step(x**2 + 1, x, .57624, .01)
  '0.564715'
  """
  return x_0 - α*diff(f,x).subs(x, x_0)
```

That takes care of one interation, now let's add a helper to perform `n` iterations.

```python
def gradient_descent(f, x, x_0, α, n=1):
  """
  Performs `n` iterations of gradient descent

  >>> "%.06f" % gradient_descent(x**2 + 1, x, .6, .01, n=3)
  '0.564715'
  """

  x_n = x_0

  for _ in range(0,n):
    x_n = gradient_descent_step(f, x, x_n, α)

  return x_n
```

Problem 2
=========

$f(x,y) = (x-7)^2 + (y-3)^2$

Before we can begin to apply the numeric portion of our algorithm, we'll need to take our partial derivatives so that we can craft a gradient to follow.

$f(x,y) = (x-7)^2 + (y-3)^2$


Problem 3
=========

f(x,y) = (x^2)/10 + 10 y^2 
