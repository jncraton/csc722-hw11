% CSC722 HW11
% Jon Craton

> The whole work for homework and finals are required. The final results are not acceptable.

> Apply the gradient descent algorithm for the following function for 3 iterations assuming learning rate alpha = 0.01 

```python, echo=False
from sympy import init_printing, latex
init_printing(use_unicode=True)

def tex(l,v=''):
  if v:
    v += ' = '

  print('$%s%s$' % (v,latex(l)))
```

Problem 1
=========

$f(x) = x^2 + 1$

As the name implies, the first step to gradient descent is calculating our gradient. For this expression that is:

$f(x) = x^2 + 1$

$f'(x) = 2x$

Initial Guess
-------------

Now, we pick a point at random. Let's start from x=.6. 

Iteration 1
-----------

We plug our current value into our derivative to get the gradient at that point:

$f'(.6) = 2(.6) = 1.2$

Then we adjust our guess based on the local gradient and learning rate as:

$x_0 = .6$

$x_{n+1} = x_n - αf'(x_n)$

$α=.01$

$x_1 = .6 - (.01)(1.2)$

```python
.6 - .01*1.2
```

Iteration 2
-----------

We plug our new value into our derivative to get the gradient at that point:

$f'(.588) = 2(.588) = 1.176$

Then we adjust our guess based on the local gradient and learning rate as:

$x_1 = .588$

$x_{n+1} = x_n - αf'(x_n)$

$α=.01$

$x_2 = .588 - (.01)(1.176)$

```python
.588 - (.01)*(1.176)
```

Iteration 3
-----------

We plug our new value into our derivative to get the gradient at that point:

$f'(.57624) = 2(.57624) =  1.15248$

Then we adjust our guess based on the local gradient and learning rate as:

$x_1 = .57624$

$x_{n+1} = x_n - αf'(x_n)$

$α=.01$

$x_2 = .57624 - (.01)(1.15248)$

```python
.57624 - (.01)*(1.15248)
```

This shows the basic concept after three iterations. We'd obviously need to continue to get close to our real minimum.

Python Implementation
---------------------

Let's implement this algorithm in Python to solidify our understanding.

```python
from sympy import Symbol, diff

x = Symbol('x')
y = Symbol('y')

def gradient_descent(f, x, x_0, α, n=1):
  """
  Performs `n` iterations of gradient descent

  >>> gradient_descent(x**2 + 1, [x], [.6], .01, n=3)
  [0.564715200000000]
  >>> gradient_descent((x-7)**2 + (y-3)**2, [x,y], [.7,.7], .01, n=3)
  [1.07049040000000, 0.835258400000000]

  With a large n, this gets us close to the true minimum (7,3) in
  the following example:
  
  >>> gradient_descent((x-7)**2 + (y-3)**2, [x,y], [.7,.7], .01, n=500)
  [6.99974154889359, 2.99990564483416]
  """

  x_n = [v0 - α*diff(f,v).subs(v, v0) for (v,v0) in zip(x, x_0)]

  if n==1: return x_n
  
  return gradient_descent(f, x, x_n, α, n=n-1)
```

Problem 2
=========

$f(x,y) = (x-7)^2 + (y-3)^2$

Before we can begin to apply the numeric portion of our algorithm, we'll need to take our partial derivatives so that we can craft a gradient to follow.

```python,results='tex'
f = (x-7)**2 + (y-3)**2
tex(f)
```

```python,results='tex'
tex(diff(f, x))
```

```python,results='tex'
tex(diff(f, y))
```

Let's also pick some starting values:

```python
x_n = .7
y_n = .7
```

Iteration 1
-----------

We plug our current value into our gradients:

$α=.01$

First for x:

$x_{n+1} = x_n - αf'(x_n)$

```python
x_n = x_n - .01*(2*x_n-14)
print(x_n)
```

Then for y:

$y_{n+1} = y_n - αf'(y_n)$

```python
y_n = y_n - .01*(2*y_n-6)
print(y_n)
```

We can quickly check this against our Python implementation:

```python
print(gradient_descent(f,[x,y],[.7,.7],α=.01,n=1))
```

Iteration 2
-----------

We plug our current value into our gradients:

$α=.01$

First for x:

$x_{n+1} = x_n - αf'(x_n)$

```python
x_n = x_n - .01*(2*x_n-14)
print(x_n)
```

Then for y:

$y_{n+1} = y_n - αf'(y_n)$

```python
y_n = y_n - .01*(2*y_n-6)
print(y_n)
```

Iteration 3
-----------

We plug our current value into our gradients:

$α=.01$

First for x:

$x_{n+1} = x_n - αf'(x_n)$

```python
x_n = x_n - .01*(2*x_n-14)
print(x_n)
```

Then for y:

$y_{n+1} = y_n - αf'(y_n)$

```python
y_n = y_n - .01*(2*y_n-6)
print(y_n)
```

Check
-----

We can check our result against our Python implementation:

```python
print(gradient_descent(f,[x,y],[.7,.7],α=.01,n=3))
```

Problem 3
=========

$f(x,y) = (x^2)/10 + 10 y^2$

Before we can begin to apply the numeric portion of our algorithm, we'll need to take our partial derivatives so that we can craft a gradient to follow.

```python,results='tex'
f = (x**2)/10 + 10*y**2
tex(f)
```

```python,results='tex'
tex(diff(f, x))
```

```python,results='tex'
tex(diff(f, y))
```

Let's also pick some starting values:

```python
x_n = 1.5
y_n = 1.5
```

Iteration 1
-----------

We plug our current value into our gradients:

$α=.01$

First for x:

$x_{n+1} = x_n - αf'(x_n)$

```python
x_n = x_n - .01*(x_n/5)
print(x_n)
```

Then for y:

$y_{n+1} = y_n - αf'(y_n)$

```python
y_n = y_n - .01*(20*y_n)
print(y_n)
```

We can quickly check this against our Python implementation:

```python
print(gradient_descent(f,[x,y],[1.5,1.5],α=.01,n=1))
```

Iteration 2
-----------

We plug our current value into our gradients:

$α=.01$

First for x:

$x_{n+1} = x_n - αf'(x_n)$

```python
x_n = x_n - .01*(x_n/5)
print(x_n)
```

Then for y:

$y_{n+1} = y_n - αf'(y_n)$

```python
y_n = y_n - .01*(20*y_n)
print(y_n)
```

Iteration 3
-----------

We plug our current value into our gradients:

$α=.01$

First for x:

$x_{n+1} = x_n - αf'(x_n)$

```python
x_n = x_n - .01*(x_n/5)
print(x_n)
```

Then for y:

$y_{n+1} = y_n - αf'(y_n)$

```python
y_n = y_n - .01*(20*y_n)
print(y_n)
```

Check
-----

We can check our result against our Python implementation:

```python
print(gradient_descent(f,[x,y],[1.5,1.5],α=.01,n=3))
```
